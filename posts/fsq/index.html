<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>VQ-VAE vs. FSQ | Akshay Vegesna</title>
<meta name="keywords" content="">
<meta name="description" content="VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn&rsquo;t naturally tokenized. This is true in most production models for image, video, and audio generation1.
In this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)2 which works better on the MNIST dataset.">
<meta name="author" content="">
<link rel="canonical" href="http://akshayvegesna.github.io/posts/fsq/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css" integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://akshayvegesna.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://akshayvegesna.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://akshayvegesna.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://akshayvegesna.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="http://akshayvegesna.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<meta property="og:title" content="VQ-VAE vs. FSQ" />
<meta property="og:description" content="VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn&rsquo;t naturally tokenized. This is true in most production models for image, video, and audio generation1.
In this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)2 which works better on the MNIST dataset." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://akshayvegesna.github.io/posts/fsq/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-10-27T00:48:24-07:00" />
<meta property="article:modified_time" content="2025-10-27T00:48:24-07:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="VQ-VAE vs. FSQ"/>
<meta name="twitter:description" content="VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn&rsquo;t naturally tokenized. This is true in most production models for image, video, and audio generation1.
In this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)2 which works better on the MNIST dataset."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://akshayvegesna.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VQ-VAE vs. FSQ",
      "item": "http://akshayvegesna.github.io/posts/fsq/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VQ-VAE vs. FSQ",
  "name": "VQ-VAE vs. FSQ",
  "description": "VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn\u0026rsquo;t naturally tokenized. This is true in most production models for image, video, and audio generation1.\nIn this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)2 which works better on the MNIST dataset.",
  "keywords": [
    
  ],
  "articleBody": "VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn’t naturally tokenized. This is true in most production models for image, video, and audio generation1.\nIn this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)2 which works better on the MNIST dataset. FSQ avoids the learnable codebook and complex loss balancing in VQ-VAE, resulting in better reconstruction and codebook utilization with simpler training. This post is simply to popularize the alternative FSQ which I have seen work better in my own research.\nVQ-VAE VQ-VAE learns a discrete codebook of embeddings to tokenize continuous data. The encoder produces a continuous representation, then we find the nearest codebook embedding for each position—this gives us discrete tokens.\nThe core challenge is training the codebook effectively. We need the codebook embeddings to be useful for reconstruction, but we also need the encoder to actually use them. This requires three losses working together: reconstruction, codebook, and commitment. The reconstruction loss is straightforward—standard MSE between input and decoded output. The codebook loss moves embeddings toward encoder outputs, while the commitment loss pulls the encoder toward the codebook, ensuring both components learn together rather than drifting independently.\nHere’s the implementation:\ndef forward(self, z: torch.Tensor) -\u003e tuple[torch.Tensor, dict]: z_flat = rearrange(z, 'b c h w -\u003e (b h w) c') # Find nearest codebook embedding for each spatial position distances = (z_flat.pow(2).sum(1, keepdim=True) + self.embeddings.weight.pow(2).sum(1) - 2 * z_flat @ self.embeddings.weight.t()) indices = distances.argmin(1) # Discrete codes quantized = self.embeddings(indices) # Look up embeddings quantized = rearrange(quantized, '(b h w) c -\u003e b c h w', ...) # Two losses: move codebook toward encoder, and vice versa commitment_loss = F.mse_loss(quantized.detach(), z) codebook_loss = F.mse_loss(quantized, z.detach()) # Straight-through: use quantized in forward, copy gradients in backward quantized = z + (quantized - z).detach() return quantized, {...} The main challenge with VQ-VAE is balancing these losses and avoiding codebook collapse (where only a few embeddings get used). FSQ offers a simpler alternative.\nFSQ Finite Scalar Quantization (FSQ) is a simpler alternative that eliminates the learnable codebook entirely. Instead of learning embedding vectors, FSQ quantizes each dimension independently to fixed levels. For example, levels=[8,8,8] gives 8 × 8 × 8 = 512 codes without any learnable embedding parameters.\nLet’s look at the FSQQuantizer.forward() method:\ndef forward(self, z: torch.Tensor) -\u003e tuple[torch.Tensor, dict]: z = rearrange(z, 'b c h w -\u003e b h w c') z = self.project_in(z) # Project to codebook_dim if needed # Bound and quantize with straight-through eps = 1e-3 half_l = (self._levels - 1) * (1 + eps) / 2 offset = torch.where(self._levels % 2 == 0, 0.5, 0.0) shift = torch.atanh(offset / half_l) bounded = torch.tanh(z + shift) * half_l - offset codes = (bounded.round() + (bounded - bounded.detach())) / (self._levels // 2) out = self.project_out(codes) out = rearrange(out, 'b h w c -\u003e b c h w') return out, {'indices': flat_indices, 'losses': {}, ...} The encoder produces a continuous representation, which is projected to the codebook dimension. Each dimension is then independently bounded using tanh and rounded to one of its fixed levels. With levels=[8,8,8], each of the 3 dimensions can take 8 discrete values, giving 512 total codes. The straight-through estimator (bounded.round() + (bounded - bounded.detach())) enables gradient flow: use discrete codes in the forward pass, but copy gradients through the continuous values during backpropagation.\nThe main advantage of FSQ over VQ-VAE is simplicity: no codebook loss, no commitment loss, no learnable embeddings to balance. The encoder simply learns to produce values that reconstruct well when quantized to fixed levels. This avoids codebook collapse and requires fewer hyperparameters.\nResults You can see that the reconstruction error is lower and the codebook utilization is much higher on the validation set using FSQ versus VQ-VAE on the following plots (using the training code below).\nWhile MNIST is a simple dataset, these results demonstrate why FSQ is preferable: simpler training with better codebook utilization.\nFull Code Reference The complete implementation code for both VQ-VAE and FSQ quantizers, including the training script and experiments on MNIST, is available as a GitHub Gist here.\nVan Den Oord, A., Vinyals, O., \u0026 Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. Advances in Neural Information Processing Systems (NIPS 2017). arXiv:1711.00937 ↩︎\nMentzer, F., Minnen, D., Agustsson, E., \u0026 Tschannen, M. (2023). Finite Scalar Quantization: VQ-VAE Made Simple. arXiv:2309.15505 ↩︎\n",
  "wordCount" : "743",
  "inLanguage": "en",
  "datePublished": "2025-10-27T00:48:24-07:00",
  "dateModified": "2025-10-27T00:48:24-07:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://akshayvegesna.github.io/posts/fsq/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akshay Vegesna",
    "logo": {
      "@type": "ImageObject",
      "url": "http://akshayvegesna.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://akshayvegesna.github.io/" accesskey="h" title="Akshay Vegesna (Alt + H)">Akshay Vegesna</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      VQ-VAE vs. FSQ
    </h1>
    <div class="post-meta"><span title='2025-10-27 00:48:24 -0700 PDT'>October 27, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>VQ-VAE (Vector-Quantized Variational Autoencoder) is a standard approach in the ML literature for quantizing data. Quantizing data is critical in any situation where we want to use an autoregressive transformer model on data which isn&rsquo;t naturally tokenized. This is true in most production models for image, video, and audio generation<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>In this blog post we demonstrate an alternative to VQ-VAE named FSQ (Finite Scalar Quantization)<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> which works better on the MNIST dataset. FSQ avoids the learnable codebook and complex loss balancing in VQ-VAE, resulting in better reconstruction and codebook utilization with simpler training. This post is simply to popularize the alternative FSQ which I have seen work better in my own research.</p>
<h3 id="vq-vae">VQ-VAE<a hidden class="anchor" aria-hidden="true" href="#vq-vae">#</a></h3>
<p>VQ-VAE learns a discrete codebook of embeddings to tokenize continuous data. The encoder produces a continuous representation, then we find the nearest codebook embedding for each position—this gives us discrete tokens.</p>
<p>The core challenge is training the codebook effectively. We need the codebook embeddings to be useful for reconstruction, but we also need the encoder to actually use them. This requires three losses working together: reconstruction, codebook, and commitment. The reconstruction loss is straightforward—standard MSE between input and decoded output. The codebook loss moves embeddings toward encoder outputs, while the commitment loss pulls the encoder toward the codebook, ensuring both components learn together rather than drifting independently.</p>
<p>Here&rsquo;s the implementation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, z: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> tuple[torch<span style="color:#f92672">.</span>Tensor, dict]:
</span></span><span style="display:flex;"><span>    z_flat <span style="color:#f92672">=</span> rearrange(z, <span style="color:#e6db74">&#39;b c h w -&gt; (b h w) c&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find nearest codebook embedding for each spatial position</span>
</span></span><span style="display:flex;"><span>    distances <span style="color:#f92672">=</span> (z_flat<span style="color:#f92672">.</span>pow(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) 
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>embeddings<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>pow(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> z_flat <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>embeddings<span style="color:#f92672">.</span>weight<span style="color:#f92672">.</span>t())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    indices <span style="color:#f92672">=</span> distances<span style="color:#f92672">.</span>argmin(<span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Discrete codes</span>
</span></span><span style="display:flex;"><span>    quantized <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embeddings(indices)  <span style="color:#75715e"># Look up embeddings</span>
</span></span><span style="display:flex;"><span>    quantized <span style="color:#f92672">=</span> rearrange(quantized, <span style="color:#e6db74">&#39;(b h w) c -&gt; b c h w&#39;</span>, <span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Two losses: move codebook toward encoder, and vice versa</span>
</span></span><span style="display:flex;"><span>    commitment_loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(quantized<span style="color:#f92672">.</span>detach(), z)
</span></span><span style="display:flex;"><span>    codebook_loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(quantized, z<span style="color:#f92672">.</span>detach())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Straight-through: use quantized in forward, copy gradients in backward</span>
</span></span><span style="display:flex;"><span>    quantized <span style="color:#f92672">=</span> z <span style="color:#f92672">+</span> (quantized <span style="color:#f92672">-</span> z)<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> quantized, {<span style="color:#f92672">...</span>}
</span></span></code></pre></div><p>The main challenge with VQ-VAE is balancing these losses and avoiding codebook collapse (where only a few embeddings get used). FSQ offers a simpler alternative.</p>
<h3 id="fsq">FSQ<a hidden class="anchor" aria-hidden="true" href="#fsq">#</a></h3>
<p>Finite Scalar Quantization (FSQ) is a simpler alternative that eliminates the learnable codebook entirely. Instead of learning embedding vectors, FSQ quantizes each dimension independently to fixed levels. For example, <code>levels=[8,8,8]</code> gives 8 × 8 × 8 = 512 codes without any learnable embedding parameters.</p>
<p>Let&rsquo;s look at the <code>FSQQuantizer.forward()</code> method:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, z: torch<span style="color:#f92672">.</span>Tensor) <span style="color:#f92672">-&gt;</span> tuple[torch<span style="color:#f92672">.</span>Tensor, dict]:
</span></span><span style="display:flex;"><span>    z <span style="color:#f92672">=</span> rearrange(z, <span style="color:#e6db74">&#39;b c h w -&gt; b h w c&#39;</span>)
</span></span><span style="display:flex;"><span>    z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>project_in(z)  <span style="color:#75715e"># Project to codebook_dim if needed</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Bound and quantize with straight-through</span>
</span></span><span style="display:flex;"><span>    eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-3</span>
</span></span><span style="display:flex;"><span>    half_l <span style="color:#f92672">=</span> (self<span style="color:#f92672">.</span>_levels <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> eps) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    offset <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>where(self<span style="color:#f92672">.</span>_levels <span style="color:#f92672">%</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>    shift <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>atanh(offset <span style="color:#f92672">/</span> half_l)
</span></span><span style="display:flex;"><span>    bounded <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tanh(z <span style="color:#f92672">+</span> shift) <span style="color:#f92672">*</span> half_l <span style="color:#f92672">-</span> offset
</span></span><span style="display:flex;"><span>    codes <span style="color:#f92672">=</span> (bounded<span style="color:#f92672">.</span>round() <span style="color:#f92672">+</span> (bounded <span style="color:#f92672">-</span> bounded<span style="color:#f92672">.</span>detach())) <span style="color:#f92672">/</span> (self<span style="color:#f92672">.</span>_levels <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>project_out(codes)
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> rearrange(out, <span style="color:#e6db74">&#39;b h w c -&gt; b c h w&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> out, {<span style="color:#e6db74">&#39;indices&#39;</span>: flat_indices, <span style="color:#e6db74">&#39;losses&#39;</span>: {}, <span style="color:#f92672">...</span>}
</span></span></code></pre></div><p>The encoder produces a continuous representation, which is projected to the codebook dimension. Each dimension is then independently bounded using <code>tanh</code> and rounded to one of its fixed levels. With <code>levels=[8,8,8]</code>, each of the 3 dimensions can take 8 discrete values, giving 512 total codes. The straight-through estimator <code>(bounded.round() + (bounded - bounded.detach()))</code> enables gradient flow: use discrete codes in the forward pass, but copy gradients through the continuous values during backpropagation.</p>
<p>The main advantage of FSQ over VQ-VAE is simplicity: no codebook loss, no commitment loss, no learnable embeddings to balance. The encoder simply learns to produce values that reconstruct well when quantized to fixed levels. This avoids codebook collapse and requires fewer hyperparameters.</p>
<h3 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h3>
<p>You can see that the reconstruction error is lower and the codebook utilization is much higher on the validation set using FSQ versus VQ-VAE on the following plots (using the training code below).</p>
<p><img loading="lazy" src="/images/vqvae_fsq_plots.png" alt="FSQ"  />
</p>
<p>While MNIST is a simple dataset, these results demonstrate why FSQ is preferable: simpler training with better codebook utilization.</p>
<h3 id="full-code-reference">Full Code Reference<a hidden class="anchor" aria-hidden="true" href="#full-code-reference">#</a></h3>
<p>The complete implementation code for both VQ-VAE and FSQ quantizers, including the training script and experiments on MNIST, is available as a GitHub Gist <a href="https://gist.github.com/akshayvegesna/51aa766d29ad027252f55218093b9b15">here</a>.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Van Den Oord, A., Vinyals, O., &amp; Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. <em>Advances in Neural Information Processing Systems</em> (NIPS 2017). <a href="https://arxiv.org/abs/1711.00937">arXiv:1711.00937</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Mentzer, F., Minnen, D., Agustsson, E., &amp; Tschannen, M. (2023). Finite Scalar Quantization: VQ-VAE Made Simple. <a href="https://arxiv.org/abs/2309.15505">arXiv:2309.15505</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
